<<<<<<< HEAD
# this is of length nlambda.beta*nlambda.gamma i.e. one set of y's for each tuning parameter
y_tilde_2_list <- mapply("-", y_tilde_2_list_temp, term_2_temp_list, SIMPLIFY = F)
# j' less than j
j.prime.less <- index[which(index[,"index"] < index[which(index$main.effect.names == j),2]),
"main.effect.names"]
# the if conditions in term1 and term2 are to check if there are
# any variables greater or less than j
# lapply is faster than mclapply here
term_1_list_not_converged <- if (length(j.prime.less) != 0) {
lapply(seq_len(nlambda)[which(converged==0)], function(i)
x[,paste(j.prime.less,j,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j.prime.less,j,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.less, , drop = F]))} else
matrix(rep(0,length(beta_hat_next_list[which(converged==0)])), ncol = 1)
term_1_list <- replace(term_1_list, which(converged==0), term_1_list_not_converged)
# j' greater than j
j.prime.greater <- index[which(index[,"index"] > index[which(index$main.effect.names == j),2]),
"main.effect.names"]
term_2_list_not_converged <- if (length(j.prime.greater) != 0) {
lapply(seq_len(nlambda)[which(converged==0)], function(i)
x[,paste(j,j.prime.greater,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j, j.prime.greater,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.greater,,drop = F])) } else
matrix(rep(0,length(beta_hat_next_list[which(converged==0)])), ncol = 1)
term_2_list <- replace(term_2_list, which(converged==0), term_2_list_not_converged)
# lapply is faster than mclapply
x_tilde_2_list <- lapply(seq_len(length(term_1_list)),
function(i) x[,j, drop = F] +
term_1_list[[i]] + term_2_list[[i]])
# glmnet is giving weired results for this... and is slower than using my
# soft function. use this. non-parallel version is faster
# the result of this should give 1 beta for each tuningn parameter
# This calculates for all tuning parameters
beta_hat_next_list_j <- lapply(seq_len(length(x_tilde_2_list)), function(i)
soft(x = x_tilde_2_list[[i]],
y = y_tilde_2_list[[i]],
weight = adaptive_weights_list[[i]][j,,drop=F],
lambda = lambda_beta_list[[i]]))
# update beta_j for each tuning parameter but only those that
# have not converged
for (i in seq_len(nlambda)[which(converged==0)]) {
beta_hat_next_list[[i]][j,] <- beta_hat_next_list_j[[i]]
}
}
# Q[m+1,which(converged==0)] <- parallel::mcmapply(Q_theta,
#                               beta = beta_hat_next_list[which(converged==0)],
#                               gamma = gamma_hat_next_list[which(converged==0)],
#                               lambda.beta = lambda_beta_list[which(converged==0)],
#                               lambda.gamma = lambda_gamma_list[which(converged==0)],
#                               MoreArgs = list(x = x, y = y,
#                                               weights = adaptive.weights,
#                                               main.effect.names = main.effect.names,
#                                               interaction.names = interaction.names),
#                               mc.cores = cores)
Q[m+1,] <- parallel::mcmapply(Q_theta,
beta = beta_hat_next_list,
gamma = gamma_hat_next_list,
lambda.beta = lambda_beta_list,
lambda.gamma = lambda_gamma_list,
weights = adaptive_weights_list,
MoreArgs = list(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names),
mc.cores = cores)
#betas[,m+1,] <- beta_hat_next_list
#gammas[,m+1,] <- gamma_hat_next_list
delta <- abs(Q[m,] - Q[m+1,])/abs(Q[m,])
converged <- as.numeric(delta<threshold)
print(paste("Iteration:",m, ", Q(theta):",Q[m+1,2]))
print(paste(converged))
m = m + 1
beta_hat_previous_list <- beta_hat_next_list
# adaptive weight for each tuning parameter. currently this is the
# same for iterations, but I am coding it here
# for flexibility in case we want to change the weights at each iteration
# adaptive_weights_list <- lapply(x_tilde_list, function(i)
#     adaptive.weights[colnames(i),,drop = F])
#
# colnames(x_tilde_list[[1]])
adaptive_weights_list_not_converged <- lapply(seq_len(nlambda)[which(converged==0)],
function(i)
update_weights(betas = beta_hat_previous_list[[i]],
gammas = gamma_hat_previous_list[[i]],
main.effect.names = main.effect.names,
interaction.names = interaction.names))
adaptive_weights_list <- replace(adaptive_weights_list, which(converged==0), adaptive_weights_list_not_converged)
for (j in which(converged==0)) {
y_tilde_list[[j]] <- y - x[,main.effect.names,drop = F] %*% beta_hat_previous_list[[j]]
}
# calculate x_tilde for each beta vector corresponding to a diffent tuning parameter
# x_tilde_list <- lapply(beta_hat_previous_list[which(converged==0)],
#                        function(i) xtilde(interaction.names = interaction.names,
#                                           data.main.effects = x[,main.effect.names, drop = F],
#                                           beta.main.effects = i))
for (j in which(converged==0)) {
x_tilde_list[[j]] <- xtilde(interaction.names = interaction.names,
data.main.effects = x[,main.effect.names, drop = F],
beta.main.effects = beta_hat_previous_list[[j]])
}
# indices of the x_tilde matrices that have all 0 columns
zero_x_tilde <- which(sapply(x_tilde_list,
function(i) is.null(colnames(check_col_0(i)))))
# this will store the results but will be shorter than nlambda
gamma_hat_next_list_not_converged <- parallel::mclapply(seq_len(nlambda)[which(converged==0)],
function(i) {
if (i %in% zero_x_tilde) coef_zero_gamma_matrix else
as.matrix(coef(glmnet::glmnet(
x = x_tilde_list[[i]],
y = y_tilde_list[[i]],
penalty.factor = adaptive_weights_list[[i]][interaction.names,,drop=F],
lambda = lambda_gamma_list[[i]],
standardize = F, intercept = F))[-1,,drop = F])},
mc.cores = cores)
# k = 1
# for (j in which(converged==0)) {
#     gamma_hat_next_list[[j]] <- gamma_hat_next_list_tmp[[k]]
#     k = k + 1
# }
gamma_hat_next_list <- replace(gamma_hat_next_list, which(converged==0), gamma_hat_next_list_not_converged)
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# update beta (main effect parameter) step 4 of algortihm in Choi et al
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
beta_hat_next_list <- beta_hat_previous_list
for (j in main.effect.names) {
#j = "x5"
#print(paste(j))
# determine the main effects not in j
j_prime_not_in_j <- dplyr::setdiff(main.effect.names,j)
# y_tilde_2_list_temp <- lapply(beta_hat_next_list[which(converged==0)], function(i) y -
#                                   x[,j_prime_not_in_j, drop = F] %*% i[j_prime_not_in_j, , drop = F])
for (notconverged in which(converged==0)) {
y_tilde_2_list_temp[[notconverged]] <- y -
x[,j_prime_not_in_j, drop = F] %*% beta_hat_next_list[[notconverged]][j_prime_not_in_j, , drop = F]
}
# length(y_tilde_2_list_temp)
# mclapply is faster than lapply even with just two cores
term_2_temp_list_not_converged <- parallel::mclapply(seq_len(nlambda)[which(converged==0)], function(i)
as.matrix(
rowSums(
xtilde_mod(beta.main.effects = beta_hat_next_list[[i]][j_prime_not_in_j, , drop = F],
gamma.interaction.effects = gamma_hat_next_list[[i]],
interaction.names = interaction.names[-grep(j, interaction.names)],
data.main.effects = x[,j_prime_not_in_j, drop = F])
),
ncol = 1),
mc.cores = cores)
term_2_temp_list <- replace(term_2_temp_list, which(converged==0), term_2_temp_list_not_converged)
# this is of length nlambda.beta*nlambda.gamma i.e. one set of y's for each tuning parameter
y_tilde_2_list <- mapply("-", y_tilde_2_list_temp, term_2_temp_list, SIMPLIFY = F)
# j' less than j
j.prime.less <- index[which(index[,"index"] < index[which(index$main.effect.names == j),2]),
"main.effect.names"]
# the if conditions in term1 and term2 are to check if there are
# any variables greater or less than j
# lapply is faster than mclapply here
term_1_list_not_converged <- if (length(j.prime.less) != 0) {
lapply(seq_len(nlambda)[which(converged==0)], function(i)
x[,paste(j.prime.less,j,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j.prime.less,j,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.less, , drop = F]))} else
matrix(rep(0,length(beta_hat_next_list[which(converged==0)])), ncol = 1)
term_1_list <- replace(term_1_list, which(converged==0), term_1_list_not_converged)
# j' greater than j
j.prime.greater <- index[which(index[,"index"] > index[which(index$main.effect.names == j),2]),
"main.effect.names"]
term_2_list_not_converged <- if (length(j.prime.greater) != 0) {
lapply(seq_len(nlambda)[which(converged==0)], function(i)
x[,paste(j,j.prime.greater,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j, j.prime.greater,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.greater,,drop = F])) } else
matrix(rep(0,length(beta_hat_next_list[which(converged==0)])), ncol = 1)
term_2_list <- replace(term_2_list, which(converged==0), term_2_list_not_converged)
# lapply is faster than mclapply
x_tilde_2_list <- lapply(seq_len(length(term_1_list)),
function(i) x[,j, drop = F] +
term_1_list[[i]] + term_2_list[[i]])
# glmnet is giving weired results for this... and is slower than using my
# soft function. use this. non-parallel version is faster
# the result of this should give 1 beta for each tuningn parameter
# This calculates for all tuning parameters
beta_hat_next_list_j <- lapply(seq_len(length(x_tilde_2_list)), function(i)
soft(x = x_tilde_2_list[[i]],
y = y_tilde_2_list[[i]],
weight = adaptive_weights_list[[i]][j,,drop=F],
lambda = lambda_beta_list[[i]]))
# update beta_j for each tuning parameter but only those that
# have not converged
for (i in seq_len(nlambda)[which(converged==0)]) {
beta_hat_next_list[[i]][j,] <- beta_hat_next_list_j[[i]]
}
}
# Q[m+1,which(converged==0)] <- parallel::mcmapply(Q_theta,
#                               beta = beta_hat_next_list[which(converged==0)],
#                               gamma = gamma_hat_next_list[which(converged==0)],
#                               lambda.beta = lambda_beta_list[which(converged==0)],
#                               lambda.gamma = lambda_gamma_list[which(converged==0)],
#                               MoreArgs = list(x = x, y = y,
#                                               weights = adaptive.weights,
#                                               main.effect.names = main.effect.names,
#                                               interaction.names = interaction.names),
#                               mc.cores = cores)
Q[m+1,] <- parallel::mcmapply(Q_theta,
beta = beta_hat_next_list,
gamma = gamma_hat_next_list,
lambda.beta = lambda_beta_list,
lambda.gamma = lambda_gamma_list,
weights = adaptive_weights_list,
MoreArgs = list(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names),
mc.cores = cores)
#betas[,m+1,] <- beta_hat_next_list
#gammas[,m+1,] <- gamma_hat_next_list
delta <- abs(Q[m,] - Q[m+1,])/abs(Q[m,])
converged <- as.numeric(delta<threshold)
print(paste("Iteration:",m, ", Q(theta):",Q[m+1,2]))
print(paste(converged))
m = m + 1
beta_hat_previous_list <- beta_hat_next_list
# adaptive weight for each tuning parameter. currently this is the
# same for iterations, but I am coding it here
# for flexibility in case we want to change the weights at each iteration
# adaptive_weights_list <- lapply(x_tilde_list, function(i)
#     adaptive.weights[colnames(i),,drop = F])
#
# colnames(x_tilde_list[[1]])
adaptive_weights_list_not_converged <- lapply(seq_len(nlambda)[which(converged==0)],
function(i)
update_weights(betas = beta_hat_previous_list[[i]],
gammas = gamma_hat_previous_list[[i]],
main.effect.names = main.effect.names,
interaction.names = interaction.names))
adaptive_weights_list <- replace(adaptive_weights_list, which(converged==0), adaptive_weights_list_not_converged)
for (j in which(converged==0)) {
y_tilde_list[[j]] <- y - x[,main.effect.names,drop = F] %*% beta_hat_previous_list[[j]]
}
# calculate x_tilde for each beta vector corresponding to a diffent tuning parameter
# x_tilde_list <- lapply(beta_hat_previous_list[which(converged==0)],
#                        function(i) xtilde(interaction.names = interaction.names,
#                                           data.main.effects = x[,main.effect.names, drop = F],
#                                           beta.main.effects = i))
for (j in which(converged==0)) {
x_tilde_list[[j]] <- xtilde(interaction.names = interaction.names,
data.main.effects = x[,main.effect.names, drop = F],
beta.main.effects = beta_hat_previous_list[[j]])
}
# indices of the x_tilde matrices that have all 0 columns
zero_x_tilde <- which(sapply(x_tilde_list,
function(i) is.null(colnames(check_col_0(i)))))
# this will store the results but will be shorter than nlambda
gamma_hat_next_list_not_converged <- parallel::mclapply(seq_len(nlambda)[which(converged==0)],
function(i) {
if (i %in% zero_x_tilde) coef_zero_gamma_matrix else
as.matrix(coef(glmnet::glmnet(
x = x_tilde_list[[i]],
y = y_tilde_list[[i]],
penalty.factor = adaptive_weights_list[[i]][interaction.names,,drop=F],
lambda = lambda_gamma_list[[i]],
standardize = F, intercept = F))[-1,,drop = F])},
mc.cores = cores)
# k = 1
# for (j in which(converged==0)) {
#     gamma_hat_next_list[[j]] <- gamma_hat_next_list_tmp[[k]]
#     k = k + 1
# }
gamma_hat_next_list <- replace(gamma_hat_next_list, which(converged==0), gamma_hat_next_list_not_converged)
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# update beta (main effect parameter) step 4 of algortihm in Choi et al
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
beta_hat_next_list <- beta_hat_previous_list
for (j in main.effect.names) {
#j = "x5"
#print(paste(j))
# determine the main effects not in j
j_prime_not_in_j <- dplyr::setdiff(main.effect.names,j)
# y_tilde_2_list_temp <- lapply(beta_hat_next_list[which(converged==0)], function(i) y -
#                                   x[,j_prime_not_in_j, drop = F] %*% i[j_prime_not_in_j, , drop = F])
for (notconverged in which(converged==0)) {
y_tilde_2_list_temp[[notconverged]] <- y -
x[,j_prime_not_in_j, drop = F] %*% beta_hat_next_list[[notconverged]][j_prime_not_in_j, , drop = F]
}
# length(y_tilde_2_list_temp)
# mclapply is faster than lapply even with just two cores
term_2_temp_list_not_converged <- parallel::mclapply(seq_len(nlambda)[which(converged==0)], function(i)
as.matrix(
rowSums(
xtilde_mod(beta.main.effects = beta_hat_next_list[[i]][j_prime_not_in_j, , drop = F],
gamma.interaction.effects = gamma_hat_next_list[[i]],
interaction.names = interaction.names[-grep(j, interaction.names)],
data.main.effects = x[,j_prime_not_in_j, drop = F])
),
ncol = 1),
mc.cores = cores)
term_2_temp_list <- replace(term_2_temp_list, which(converged==0), term_2_temp_list_not_converged)
# this is of length nlambda.beta*nlambda.gamma i.e. one set of y's for each tuning parameter
y_tilde_2_list <- mapply("-", y_tilde_2_list_temp, term_2_temp_list, SIMPLIFY = F)
# j' less than j
j.prime.less <- index[which(index[,"index"] < index[which(index$main.effect.names == j),2]),
"main.effect.names"]
# the if conditions in term1 and term2 are to check if there are
# any variables greater or less than j
# lapply is faster than mclapply here
term_1_list_not_converged <- if (length(j.prime.less) != 0) {
lapply(seq_len(nlambda)[which(converged==0)], function(i)
x[,paste(j.prime.less,j,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j.prime.less,j,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.less, , drop = F]))} else
matrix(rep(0,length(beta_hat_next_list[which(converged==0)])), ncol = 1)
term_1_list <- replace(term_1_list, which(converged==0), term_1_list_not_converged)
# j' greater than j
j.prime.greater <- index[which(index[,"index"] > index[which(index$main.effect.names == j),2]),
"main.effect.names"]
term_2_list_not_converged <- if (length(j.prime.greater) != 0) {
lapply(seq_len(nlambda)[which(converged==0)], function(i)
x[,paste(j,j.prime.greater,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j, j.prime.greater,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.greater,,drop = F])) } else
matrix(rep(0,length(beta_hat_next_list[which(converged==0)])), ncol = 1)
term_2_list <- replace(term_2_list, which(converged==0), term_2_list_not_converged)
# lapply is faster than mclapply
x_tilde_2_list <- lapply(seq_len(length(term_1_list)),
function(i) x[,j, drop = F] +
term_1_list[[i]] + term_2_list[[i]])
# glmnet is giving weired results for this... and is slower than using my
# soft function. use this. non-parallel version is faster
# the result of this should give 1 beta for each tuningn parameter
# This calculates for all tuning parameters
beta_hat_next_list_j <- lapply(seq_len(length(x_tilde_2_list)), function(i)
soft(x = x_tilde_2_list[[i]],
y = y_tilde_2_list[[i]],
weight = adaptive_weights_list[[i]][j,,drop=F],
lambda = lambda_beta_list[[i]]))
# update beta_j for each tuning parameter but only those that
# have not converged
for (i in seq_len(nlambda)[which(converged==0)]) {
beta_hat_next_list[[i]][j,] <- beta_hat_next_list_j[[i]]
}
}
# Q[m+1,which(converged==0)] <- parallel::mcmapply(Q_theta,
#                               beta = beta_hat_next_list[which(converged==0)],
#                               gamma = gamma_hat_next_list[which(converged==0)],
#                               lambda.beta = lambda_beta_list[which(converged==0)],
#                               lambda.gamma = lambda_gamma_list[which(converged==0)],
#                               MoreArgs = list(x = x, y = y,
#                                               weights = adaptive.weights,
#                                               main.effect.names = main.effect.names,
#                                               interaction.names = interaction.names),
#                               mc.cores = cores)
Q[m+1,] <- parallel::mcmapply(Q_theta,
beta = beta_hat_next_list,
gamma = gamma_hat_next_list,
lambda.beta = lambda_beta_list,
lambda.gamma = lambda_gamma_list,
weights = adaptive_weights_list,
MoreArgs = list(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names),
mc.cores = cores)
#betas[,m+1,] <- beta_hat_next_list
#gammas[,m+1,] <- gamma_hat_next_list
delta <- abs(Q[m,] - Q[m+1,])/abs(Q[m,])
converged <- as.numeric(delta<threshold)
print(paste("Iteration:",m, ", Q(theta):",Q[m+1,2]))
print(paste(converged))
m = m + 1
beta_hat_previous_list <- beta_hat_next_list
# adaptive weight for each tuning parameter. currently this is the
# same for iterations, but I am coding it here
# for flexibility in case we want to change the weights at each iteration
# adaptive_weights_list <- lapply(x_tilde_list, function(i)
#     adaptive.weights[colnames(i),,drop = F])
#
# colnames(x_tilde_list[[1]])
adaptive_weights_list_not_converged <- lapply(seq_len(nlambda)[which(converged==0)],
function(i)
update_weights(betas = beta_hat_previous_list[[i]],
gammas = gamma_hat_previous_list[[i]],
main.effect.names = main.effect.names,
interaction.names = interaction.names))
adaptive_weights_list <- replace(adaptive_weights_list, which(converged==0), adaptive_weights_list_not_converged)
rm(list = ls())
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
system.time(res <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 50 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 5, cores = 3))
133/60
rm(list = ls())
=======
options(scipen=999, digits = 10)
>>>>>>> 4dceece59402b5b0aa61f03402fef3f19cb0ef9b
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
<<<<<<< HEAD
"%ni%" <- Negate("%in%")
system.time(res <- shim_multiple(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 3))
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 3))
res$beta
res$Q
res$gamma
res$gamma[[1]]
res2$gamma[[1]]
res2$gamma[[37]]
res2$gamma[[30]]
res$gamma[[30]]
res$Q  %>% dim
rm(list = ls())
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 3))
rm(list = ls())
options(scipen=999, digits = 10)
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
x = X; y = Y; main.effect.names = main_effect_names;
interaction.names = interaction_names;
lambda.beta = NULL ; lambda.gamma = NULL
threshold = 1e-5 ; max.iter = 500 ; initialization.type = "ridge";
nlambda.gamma = 5; nlambda.beta = 10; cores = 2
rm(list = ls())
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 3))
res2$converged
res2$beta
betas <- matrix(unlist(res2$beta), ncol = length(res2$beta), byrow = TRUE)
dim(betas)
gammas <- matrix(unlist(res2$gamma), ncol = length(res2$gamma), byrow = TRUE)
dim(gammas)
matplot(t(betas), type="l")
matplot(t(gammas), type="l")
matplot(res$Q , type="l")
matplot(res2$Q , type="l")
rm(list = ls())
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 3))
250/60
betas <- matrix(unlist(res2$beta), ncol = length(res2$beta), byrow = TRUE)
dim(betas)
gammas <- matrix(unlist(res2$gamma), ncol = length(res2$gamma), byrow = TRUE)
dim(gammas)
matplot(t(betas), type="l")
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 4))
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 1))
167/60
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 50, nlambda.beta = 1, cores = 1))
# 1 core is faster than more ...
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 1))
rm(list=ls())
rm(list=ls())
library(readxl)
library(data.table)
library(magrittr)
library(pander)
library(DT)
library(lubridate)
library(Epi)
library(ggplot2)
source("functions.R")
DT <- fread("data/CompleteData01Apr15.csv")
=======
"%ni%" <- Negate("%in%")
x = X; y = Y; main.effect.names = main_effect_names;
interaction.names = interaction_names;
lambda.beta = NULL ; lambda.gamma = NULL
threshold = 1e-5 ; max.iter = 500 ; initialization.type = "ridge";
nlambda.gamma = 10; nlambda.beta = 20; cores = 2
tuning_params <- shim_once(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names,
initialization.type = "ridge",
nlambda.gamma = nlambda.gamma, nlambda.beta = nlambda.beta)
tuning_params
lambda_gamma_list <- rep(lapply(seq_len(length(tuning_params$lambda_gamma)),
function(i) tuning_params$lambda_gamma[i]),
each = nlambda.beta)
lambda_gamma_list
lambda_beta_list <- lapply(seq_len(length(unlist(tuning_params$lambda_beta))),
function(i) unlist(tuning_params$lambda_beta)[i])
lambda_beta_list
lambda_gamma_list <- rep(lapply(seq_len(length(lambda.gamma)),
function(i) lambda.gamma[i]),
each = nlambda.beta)
lambda_gamma_list
lambda_gamma_list <- rep(lapply(seq_len(length(tuning_params$lambda_gamma)),
function(i) tuning_params$lambda_gamma[i]),
each = nlambda.beta)
lambda_beta_list <- lapply(seq_len(length(unlist(tuning_params$lambda_beta))),
function(i) unlist(tuning_params$lambda_beta)[i])
lambda_beta_list
lambda_gamma_list
nlambda = nlambda.gamma * nlambda.beta
adaptive.weights <- ridge_weights(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names)
adaptive.weights
adaptive.weights <- ridge_weights(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names)
betas_and_alphas <- uni_fun(variables = colnames(x), x = x, y = y,
include.intercept = F,
type = initialization.type)
betas_and_alphas
uni_start <- convert(betas_and_alphas, main.effect.names = main.effect.names,
interaction.names = interaction.names)
uni_start
beta_hat_previous <- replicate(nlambda, uni_start[main.effect.names, , drop = F],
simplify = "matrix")
beta_hat_previous
rownames(beta_hat_previous) <- main_effect_names
gamma_hat_previous <- replicate(nlambda, uni_start[interaction.names, , drop = F],
simplify = "matrix")
gamma_hat_previous
rm(list = ls())
options(scipen=999, digits = 10)
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
x = X; y = Y; main.effect.names = main_effect_names;
interaction.names = interaction_names;
lambda.beta = NULL ; lambda.gamma = NULL
threshold = 1e-5 ; max.iter = 500 ; initialization.type = "ridge";
x = X; y = Y; main.effect.names = main_effect_names;
interaction.names = interaction_names;
lambda.beta = NULL ; lambda.gamma = NULL
threshold = 1e-5 ; max.iter = 500 ; initialization.type = "ridge";
nlambda.gamma = 1; nlambda.beta = 10; cores = 2
tuning_params <- shim_once(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names,
initialization.type = "ridge",
nlambda.gamma = nlambda.gamma, nlambda.beta = nlambda.beta)
tuning_params
nlambda.gamma = 2; nlambda.beta = 10; cores = 2
(tuning_params <- shim_once(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names,
initialization.type = "ridge",
nlambda.gamma = nlambda.gamma, nlambda.beta = nlambda.beta))
nlambda.gamma = 5; nlambda.beta = 10; cores = 2
(tuning_params <- shim_once(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names,
initialization.type = "ridge",
nlambda.gamma = nlambda.gamma, nlambda.beta = nlambda.beta))
nlambda.gamma = 5; nlambda.beta = 10; cores = 2
(tuning_params <- shim_once(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names,
initialization.type = "ridge",
nlambda.gamma = nlambda.gamma, nlambda.beta = nlambda.beta))
nlambda.gamma = 5; nlambda.beta = 10; cores = 2
(tuning_params <- shim_once(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names,
initialization.type = "ridge",
nlambda.gamma = nlambda.gamma, nlambda.beta = nlambda.beta))
rm(list = ls())
options(scipen=999, digits = 10)
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
x = X; y = Y; main.effect.names = main_effect_names;
interaction.names = interaction_names;
lambda.beta = NULL ; lambda.gamma = NULL
threshold = 1e-5 ; max.iter = 500 ; initialization.type = "ridge";
nlambda.gamma = 5; nlambda.beta = 10; cores = 2
(tuning_params <- shim_once(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names,
initialization.type = "ridge",
nlambda.gamma = nlambda.gamma, nlambda.beta = nlambda.beta))
(tuning_params <- shim_once(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names,
initialization.type = "ridge",
nlambda.gamma = nlambda.gamma, nlambda.beta = nlambda.beta))
(tuning_params <- shim_once(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names,
initialization.type = "ridge",
nlambda.gamma = nlambda.gamma, nlambda.beta = nlambda.beta))
lambda_gamma_list <- rep(lapply(seq_len(length(tuning_params$lambda_gamma)),
function(i) tuning_params$lambda_gamma[i]),
each = nlambda.beta)
lambda_beta_list <- lapply(seq_len(length(unlist(tuning_params$lambda_beta))),
function(i) unlist(tuning_params$lambda_beta)[i])
nlambda = nlambda.gamma * nlambda.beta
adaptive.weights <- ridge_weights(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names)
betas_and_alphas <- uni_fun(variables = colnames(x), x = x, y = y,
include.intercept = F,
type = initialization.type)
uni_start <- convert(betas_and_alphas, main.effect.names = main.effect.names,
interaction.names = interaction.names)
beta_hat_previous <- replicate(nlambda, uni_start[main.effect.names, , drop = F],
simplify = "matrix")
rownames(beta_hat_previous) <- main_effect_names
gamma_hat_previous <- replicate(nlambda, uni_start[interaction.names, , drop = F],
simplify = "matrix")
rownames(gamma_hat_previous) <- interaction_names
beta_hat_previous_list <- lapply(seq_len(ncol(beta_hat_previous)),
function(i) beta_hat_previous[,i, drop = F])
gamma_hat_previous_list <- lapply(seq_len(ncol(gamma_hat_previous)),
function(i) gamma_hat_previous[,i, drop = F])
gamma_hat_previous_list <- lapply(seq_len(ncol(gamma_hat_previous)),
function(i) gamma_hat_previous[,i, drop = F])
beta_hat_previous_list
gamma_hat_previous_list
m = 1 # iteration counter
delta = 1 # threshold initialization
m
delta
Q <- matrix(nrow = max.iter+1, ncol = nlambda)
Q
Q[1,] <- parallel::mcmapply(Q_theta,
beta = beta_hat_previous_list,
gamma = gamma_hat_previous_list,
lambda.beta = lambda_beta_list,
lambda.gamma = lambda_gamma_list,
MoreArgs = list(x = x, y = y,
weights = adaptive.weights,
main.effect.names = main.effect.names,
interaction.names = interaction.names),
mc.cores = cores)
Q[1,]
Q
beta_hat_previous_list
sample(c(0,1),50, rep=T)
beta_hat_previous_list[sample(c(0,1),50, rep=T)]
beta_hat_previous_list[[1]]
y_tilde_list <- lapply(beta_hat_previous_list,
function(i) y - x[,main.effect.names,drop = F] %*% i)
# calculate x_tilde for each beta vector corresponding to a diffent tuning parameter
x_tilde_list <- lapply(beta_hat_previous_list,
function(i) xtilde(interaction.names = interaction.names,
data.main.effects = x[,main.effect.names, drop = F],
beta.main.effects = i))
adaptive_weights_list <- lapply(x_tilde_list, function(i)
adaptive.weights[colnames(i),,drop = F])
zero_x_tilde <- which(sapply(x_tilde_list,
function(i) is.null(colnames(check_col_0(i)))))
coef_zero_gamma_matrix <- matrix(data = 0,
nrow = length(interaction.names),
ncol = 1,
dimnames = list(interaction.names))
gamma_hat_next_list <- parallel::mclapply(seq_len(nlambda),
function(i) {
if (i %in% zero_x_tilde) coef_zero_gamma_matrix else
as.matrix(coef(glmnet::glmnet(
x = x_tilde_list[[i]],
y = y_tilde_list[[i]],
penalty.factor = adaptive_weights_list[[i]],
lambda = lambda_gamma_list[[i]],
standardize = F, intercept = F))[-1,,drop = F])},
mc.cores = cores)
beta_hat_next_list <- beta_hat_previous_list
for (j in main.effect.names) {
#j = "x10"
#print(paste(j))
# determine the main effects not in j
j_prime_not_in_j <- dplyr::setdiff(main.effect.names,j)
y_tilde_2_list_temp <- lapply(beta_hat_next_list, function(i) y -
x[,j_prime_not_in_j, drop = F] %*% i[j_prime_not_in_j, , drop = F])
#length(y_tilde_2_list_temp)
# mclapply is faster than lapply even with just two cores
term_2_temp_list <- parallel::mclapply(seq_len(length(beta_hat_next_list)), function(i)
as.matrix(
rowSums(
xtilde_mod(beta.main.effects = beta_hat_next_list[[i]][j_prime_not_in_j, , drop = F],
gamma.interaction.effects = gamma_hat_next_list[[i]],
interaction.names = interaction.names[-grep(j, interaction.names)],
data.main.effects = x[,j_prime_not_in_j, drop = F])
),
ncol = 1),
mc.cores = cores)
# this is of length nlambda.beta*nlambda.gamma i.e. one set of y's for each tuning parameter
y_tilde_2_list <- mapply("-", y_tilde_2_list_temp, term_2_temp_list, SIMPLIFY = F)
# index data.frame to figure out which j < j'
index <- data.frame(main.effect.names, seq_along(main.effect.names),
stringsAsFactors = F)
colnames(index) <- c("main.effect.names","index")
# j' less than j
j.prime.less <- index[which(index[,"index"] < index[which(index$main.effect.names == j),2]),
"main.effect.names"]
# the if conditions in term1 and term2 are to check if there are
# any variables greater or less than j
# lapply is faster than mclapply here
term_1_list <- if (length(j.prime.less) != 0) {
lapply(seq_len(length(beta_hat_next_list)), function(i)
x[,paste(j.prime.less,j,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j.prime.less,j,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.less, , drop = F]))} else
matrix(rep(0,length(beta_hat_next_list)), ncol = 1)
# j' greater than j
j.prime.greater <- index[which(index[,"index"] > index[which(index$main.effect.names == j),2]),
"main.effect.names"]
term_2_list <- if (length(j.prime.greater) != 0) {
lapply(seq_len(length(beta_hat_next_list)), function(i)
x[,paste(j,j.prime.greater,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j, j.prime.greater,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.greater,,drop = F])) } else
matrix(rep(0,length(beta_hat_next_list)), ncol = 1)
length(term_1_list)
# lapply is faster than mclapply
x_tilde_2_list <- lapply(seq_len(length(term_1_list)),
function(i) x[,j, drop = F] +
term_1_list[[i]] + term_2_list[[i]])
# glmnet is giving weired results for this... and is slower than using my
# soft function. use this. non-parallel version is faster
beta_hat_next_list_j <- mapply(soft,
x = x_tilde_2_list,
y = y_tilde_2_list,
lambda = lambda_beta_list,
MoreArgs = list(
weight = adaptive.weights[j,]), SIMPLIFY = F)
# update beta_j for each tuning parameter
for (i in seq_len(length(beta_hat_next_list_j))) {
beta_hat_next_list[[i]][j,] <- beta_hat_next_list_j[[i]]
}
}
Q[m+1,] <- parallel::mcmapply(Q_theta,
beta = beta_hat_next_list,
gamma = gamma_hat_next_list,
lambda.beta = lambda_beta_list,
lambda.gamma = lambda_gamma_list,
MoreArgs = list(x = x, y = y,
weights = adaptive.weights,
main.effect.names = main.effect.names,
interaction.names = interaction.names),
mc.cores = cores)
Q[m+1,]
delta_vector <- abs(Q[m,] - Q[m+1,])/abs(Q[m,])
delta_vector
m = m + 1
beta_hat_previous_list <- beta_hat_next_list
beta_hat_previous_list
replicate(3, rep(0, 10, simplify = "list"))
as.list(replicate(3, rep(0, 10, simplify = "list")))
lapply(1:3,
function(i) replicate(3, rep(0, 10, simplify = "list"))[,i, drop = F])
lapply(1:3,
function(i) replicate(3, rep(0, 10, simplify = "list"))[,i, drop = F])[c(1,0,1)]
beta_hat_previous_list
beta_hat_previous_list[c(1,0,1)] <- lapply(1:3,
function(i) replicate(3, rep(0, 10, simplify = "list"))[,i, drop = F])[c(1,0,1)]
beta_hat_previous_list
length(beta_hat_previous_list)
beta_hat_previous_list
beta_hat_previous_list[c(1,0,1)] <- lapply(1:3,
function(i) replicate(3, rep(5, 10, simplify = "list"))[,i, drop = F])[c(1,0,1)]
length(beta_hat_previous_list)
beta_hat_previous_list
lapply(1:3,
function(i) replicate(3, rep(5, 10, simplify = "list"))[,i, drop = F])[c(1,0,1)]
beta_hat_previous_list[c(1,0,1)]
beta_hat_previous_list
beta_hat_previous_list[c(1,0,1)]
lapply(1:3,
function(i) replicate(3, rep(5, 10, simplify = "list"))[,i, drop = F])[c(1,0,1)]
beta_hat_previous_list
beta_hat_previous_list[c(1,0,1)]
beta_hat_previous_list[c(1,0,1)] <<- lapply(1:3,
function(i) replicate(3, rep(5, 10, simplify = "list"))[,i, drop = F])[c(1,0,1)]
beta_hat_previous_list
beta_hat_previous_list[c(1,0,1)] <<- lapply(1:3,
function(i) replicate(3, rep(5, 10, simplify = "list"))[,i, drop = F])[c(1,0,1)]
rm(list = ls())
options(scipen=999, digits = 10)
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
rm(list = ls())
options(scipen=999, digits = 10)
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
x = X; y = Y; main.effect.names = main_effect_names;
interaction.names = interaction_names;
lambda.beta = NULL ; lambda.gamma = NULL
threshold = 1e-5 ; max.iter = 500 ; initialization.type = "ridge";
nlambda.gamma = 5; nlambda.beta = 10; cores = 2
(tuning_params <- shim_once(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names,
initialization.type = "ridge",
nlambda.gamma = nlambda.gamma, nlambda.beta = nlambda.beta))
# convert to a list. each element corresponds to a value of lambda_gamma
lambda_gamma_list <- rep(lapply(seq_len(length(tuning_params$lambda_gamma)),
function(i) tuning_params$lambda_gamma[i]),
each = nlambda.beta)
lambda_beta_list <- lapply(seq_len(length(unlist(tuning_params$lambda_beta))),
function(i) unlist(tuning_params$lambda_beta)[i])
nlambda = nlambda.gamma * nlambda.beta
adaptive.weights <- ridge_weights(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names)
betas_and_alphas <- uni_fun(variables = colnames(x), x = x, y = y,
include.intercept = F,
type = initialization.type)
uni_start <- convert(betas_and_alphas, main.effect.names = main.effect.names,
interaction.names = interaction.names)
beta_hat_previous <- replicate(nlambda, uni_start[main.effect.names, , drop = F],
simplify = "matrix")
rownames(beta_hat_previous) <- main_effect_names
gamma_hat_previous <- replicate(nlambda, uni_start[interaction.names, , drop = F],
simplify = "matrix")
rownames(gamma_hat_previous) <- interaction_names
# convert gamma and beta previous to lists each element corresponds to the
# coefficients for each combination of lambda_gamma and lambda_beta
beta_hat_previous_list <- lapply(seq_len(ncol(beta_hat_previous)),
function(i) beta_hat_previous[,i, drop = F])
gamma_hat_previous_list <- lapply(seq_len(ncol(gamma_hat_previous)),
function(i) gamma_hat_previous[,i, drop = F])
m = 1 # iteration counter
delta = 1 # threshold initialization
Q <- matrix(nrow = max.iter+1, ncol = nlambda)
Q[1,] <- parallel::mcmapply(Q_theta,
beta = beta_hat_previous_list,
gamma = gamma_hat_previous_list,
lambda.beta = lambda_beta_list,
lambda.gamma = lambda_gamma_list,
MoreArgs = list(x = x, y = y,
weights = adaptive.weights,
main.effect.names = main.effect.names,
interaction.names = interaction.names),
mc.cores = cores)
Q[1,]
y_tilde_list <- lapply(beta_hat_previous_list,
function(i) y - x[,main.effect.names,drop = F] %*% i)
# calculate x_tilde for each beta vector corresponding to a diffent tuning parameter
x_tilde_list <- lapply(beta_hat_previous_list,
function(i) xtilde(interaction.names = interaction.names,
data.main.effects = x[,main.effect.names, drop = F],
beta.main.effects = i))
adaptive_weights_list <- lapply(x_tilde_list, function(i)
adaptive.weights[colnames(i),,drop = F])
zero_x_tilde <- which(sapply(x_tilde_list,
function(i) is.null(colnames(check_col_0(i)))))
coef_zero_gamma_matrix <- matrix(data = 0,
nrow = length(interaction.names),
ncol = 1,
dimnames = list(interaction.names))
zero_x_tilde
coef_zero_gamma_matrix <- matrix(data = 0,
nrow = length(interaction.names),
ncol = 1,
dimnames = list(interaction.names))
coef_zero_gamma_matrix
gamma_hat_next_list <- parallel::mclapply(seq_len(nlambda),
function(i) {
if (i %in% zero_x_tilde) coef_zero_gamma_matrix else
as.matrix(coef(glmnet::glmnet(
x = x_tilde_list[[i]],
y = y_tilde_list[[i]],
penalty.factor = adaptive_weights_list[[i]],
mc.cores = cores)
standardize = F, intercept = F))[-1,,drop = F])},
lambda = lambda_gamma_list[[i]],
gamma_hat_next_list <- parallel::mclapply(seq_len(nlambda),
function(i) {
if (i %in% zero_x_tilde) coef_zero_gamma_matrix else
as.matrix(coef(glmnet::glmnet(
x = x_tilde_list[[i]],
y = y_tilde_list[[i]],
penalty.factor = adaptive_weights_list[[i]],
lambda = lambda_gamma_list[[i]],
standardize = F, intercept = F))[-1,,drop = F])},
mc.cores = cores)
gamma_hat_next_list
beta_hat_next_list <- beta_hat_previous_list
for (j in main.effect.names) {
#j = "x10"
#print(paste(j))
# determine the main effects not in j
j_prime_not_in_j <- dplyr::setdiff(main.effect.names,j)
y_tilde_2_list_temp <- lapply(beta_hat_next_list, function(i) y -
x[,j_prime_not_in_j, drop = F] %*% i[j_prime_not_in_j, , drop = F])
#length(y_tilde_2_list_temp)
# mclapply is faster than lapply even with just two cores
term_2_temp_list <- parallel::mclapply(seq_len(length(beta_hat_next_list)), function(i)
as.matrix(
rowSums(
xtilde_mod(beta.main.effects = beta_hat_next_list[[i]][j_prime_not_in_j, , drop = F],
gamma.interaction.effects = gamma_hat_next_list[[i]],
interaction.names = interaction.names[-grep(j, interaction.names)],
data.main.effects = x[,j_prime_not_in_j, drop = F])
),
ncol = 1),
mc.cores = cores)
# this is of length nlambda.beta*nlambda.gamma i.e. one set of y's for each tuning parameter
y_tilde_2_list <- mapply("-", y_tilde_2_list_temp, term_2_temp_list, SIMPLIFY = F)
# index data.frame to figure out which j < j'
index <- data.frame(main.effect.names, seq_along(main.effect.names),
stringsAsFactors = F)
colnames(index) <- c("main.effect.names","index")
# j' less than j
j.prime.less <- index[which(index[,"index"] < index[which(index$main.effect.names == j),2]),
"main.effect.names"]
# the if conditions in term1 and term2 are to check if there are
# any variables greater or less than j
# lapply is faster than mclapply here
term_1_list <- if (length(j.prime.less) != 0) {
lapply(seq_len(length(beta_hat_next_list)), function(i)
x[,paste(j.prime.less,j,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j.prime.less,j,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.less, , drop = F]))} else
matrix(rep(0,length(beta_hat_next_list)), ncol = 1)
# j' greater than j
j.prime.greater <- index[which(index[,"index"] > index[which(index$main.effect.names == j),2]),
"main.effect.names"]
term_2_list <- if (length(j.prime.greater) != 0) {
lapply(seq_len(length(beta_hat_next_list)), function(i)
x[,paste(j,j.prime.greater,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j, j.prime.greater,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.greater,,drop = F])) } else
matrix(rep(0,length(beta_hat_next_list)), ncol = 1)
length(term_1_list)
# lapply is faster than mclapply
x_tilde_2_list <- lapply(seq_len(length(term_1_list)),
function(i) x[,j, drop = F] +
term_1_list[[i]] + term_2_list[[i]])
# glmnet is giving weired results for this... and is slower than using my
# soft function. use this. non-parallel version is faster
beta_hat_next_list_j <- mapply(soft,
x = x_tilde_2_list,
y = y_tilde_2_list,
lambda = lambda_beta_list,
MoreArgs = list(
weight = adaptive.weights[j,]), SIMPLIFY = F)
# update beta_j for each tuning parameter
for (i in seq_len(length(beta_hat_next_list_j))) {
beta_hat_next_list[[i]][j,] <- beta_hat_next_list_j[[i]]
}
}
Q[m+1,] <- parallel::mcmapply(Q_theta,
beta = beta_hat_next_list,
gamma = gamma_hat_next_list,
lambda.beta = lambda_beta_list,
lambda.gamma = lambda_gamma_list,
MoreArgs = list(x = x, y = y,
weights = adaptive.weights,
main.effect.names = main.effect.names,
interaction.names = interaction.names),
mc.cores = cores)
delta_vector <- abs(Q[m,] - Q[m+1,])/abs(Q[m,])
delta_vector
beta_hat_previous_list
c(1,3,7)
c(1,3,7)
c(1,3,7)
c(1,3,7)
c(1,3,7)
c(1,3,7)
c(1,3,7)
c(1,3,7)
c(1,3,7)
1+1
1+1
>>>>>>> 4dceece59402b5b0aa61f03402fef3f19cb0ef9b
