# this is of length nlambda.beta*nlambda.gamma i.e. one set of y's for each tuning parameter
y_tilde_2_list <- mapply("-", y_tilde_2_list_temp, term_2_temp_list, SIMPLIFY = F)
# j' less than j
j.prime.less <- index[which(index[,"index"] < index[which(index$main.effect.names == j),2]),
"main.effect.names"]
# the if conditions in term1 and term2 are to check if there are
# any variables greater or less than j
# lapply is faster than mclapply here
term_1_list_not_converged <- if (length(j.prime.less) != 0) {
lapply(seq_len(nlambda)[which(converged==0)], function(i)
x[,paste(j.prime.less,j,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j.prime.less,j,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.less, , drop = F]))} else
matrix(rep(0,length(beta_hat_next_list[which(converged==0)])), ncol = 1)
term_1_list <- replace(term_1_list, which(converged==0), term_1_list_not_converged)
# j' greater than j
j.prime.greater <- index[which(index[,"index"] > index[which(index$main.effect.names == j),2]),
"main.effect.names"]
term_2_list_not_converged <- if (length(j.prime.greater) != 0) {
lapply(seq_len(nlambda)[which(converged==0)], function(i)
x[,paste(j,j.prime.greater,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j, j.prime.greater,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.greater,,drop = F])) } else
matrix(rep(0,length(beta_hat_next_list[which(converged==0)])), ncol = 1)
term_2_list <- replace(term_2_list, which(converged==0), term_2_list_not_converged)
# lapply is faster than mclapply
x_tilde_2_list <- lapply(seq_len(length(term_1_list)),
function(i) x[,j, drop = F] +
term_1_list[[i]] + term_2_list[[i]])
# glmnet is giving weired results for this... and is slower than using my
# soft function. use this. non-parallel version is faster
# the result of this should give 1 beta for each tuningn parameter
# This calculates for all tuning parameters
beta_hat_next_list_j <- lapply(seq_len(length(x_tilde_2_list)), function(i)
soft(x = x_tilde_2_list[[i]],
y = y_tilde_2_list[[i]],
weight = adaptive_weights_list[[i]][j,,drop=F],
lambda = lambda_beta_list[[i]]))
# update beta_j for each tuning parameter but only those that
# have not converged
for (i in seq_len(nlambda)[which(converged==0)]) {
beta_hat_next_list[[i]][j,] <- beta_hat_next_list_j[[i]]
}
}
# Q[m+1,which(converged==0)] <- parallel::mcmapply(Q_theta,
#                               beta = beta_hat_next_list[which(converged==0)],
#                               gamma = gamma_hat_next_list[which(converged==0)],
#                               lambda.beta = lambda_beta_list[which(converged==0)],
#                               lambda.gamma = lambda_gamma_list[which(converged==0)],
#                               MoreArgs = list(x = x, y = y,
#                                               weights = adaptive.weights,
#                                               main.effect.names = main.effect.names,
#                                               interaction.names = interaction.names),
#                               mc.cores = cores)
Q[m+1,] <- parallel::mcmapply(Q_theta,
beta = beta_hat_next_list,
gamma = gamma_hat_next_list,
lambda.beta = lambda_beta_list,
lambda.gamma = lambda_gamma_list,
weights = adaptive_weights_list,
MoreArgs = list(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names),
mc.cores = cores)
#betas[,m+1,] <- beta_hat_next_list
#gammas[,m+1,] <- gamma_hat_next_list
delta <- abs(Q[m,] - Q[m+1,])/abs(Q[m,])
converged <- as.numeric(delta<threshold)
print(paste("Iteration:",m, ", Q(theta):",Q[m+1,2]))
print(paste(converged))
m = m + 1
beta_hat_previous_list <- beta_hat_next_list
# adaptive weight for each tuning parameter. currently this is the
# same for iterations, but I am coding it here
# for flexibility in case we want to change the weights at each iteration
# adaptive_weights_list <- lapply(x_tilde_list, function(i)
#     adaptive.weights[colnames(i),,drop = F])
#
# colnames(x_tilde_list[[1]])
adaptive_weights_list_not_converged <- lapply(seq_len(nlambda)[which(converged==0)],
function(i)
update_weights(betas = beta_hat_previous_list[[i]],
gammas = gamma_hat_previous_list[[i]],
main.effect.names = main.effect.names,
interaction.names = interaction.names))
adaptive_weights_list <- replace(adaptive_weights_list, which(converged==0), adaptive_weights_list_not_converged)
for (j in which(converged==0)) {
y_tilde_list[[j]] <- y - x[,main.effect.names,drop = F] %*% beta_hat_previous_list[[j]]
}
# calculate x_tilde for each beta vector corresponding to a diffent tuning parameter
# x_tilde_list <- lapply(beta_hat_previous_list[which(converged==0)],
#                        function(i) xtilde(interaction.names = interaction.names,
#                                           data.main.effects = x[,main.effect.names, drop = F],
#                                           beta.main.effects = i))
for (j in which(converged==0)) {
x_tilde_list[[j]] <- xtilde(interaction.names = interaction.names,
data.main.effects = x[,main.effect.names, drop = F],
beta.main.effects = beta_hat_previous_list[[j]])
}
# indices of the x_tilde matrices that have all 0 columns
zero_x_tilde <- which(sapply(x_tilde_list,
function(i) is.null(colnames(check_col_0(i)))))
# this will store the results but will be shorter than nlambda
gamma_hat_next_list_not_converged <- parallel::mclapply(seq_len(nlambda)[which(converged==0)],
function(i) {
if (i %in% zero_x_tilde) coef_zero_gamma_matrix else
as.matrix(coef(glmnet::glmnet(
x = x_tilde_list[[i]],
y = y_tilde_list[[i]],
penalty.factor = adaptive_weights_list[[i]][interaction.names,,drop=F],
lambda = lambda_gamma_list[[i]],
standardize = F, intercept = F))[-1,,drop = F])},
mc.cores = cores)
# k = 1
# for (j in which(converged==0)) {
#     gamma_hat_next_list[[j]] <- gamma_hat_next_list_tmp[[k]]
#     k = k + 1
# }
gamma_hat_next_list <- replace(gamma_hat_next_list, which(converged==0), gamma_hat_next_list_not_converged)
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# update beta (main effect parameter) step 4 of algortihm in Choi et al
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
beta_hat_next_list <- beta_hat_previous_list
for (j in main.effect.names) {
#j = "x5"
#print(paste(j))
# determine the main effects not in j
j_prime_not_in_j <- dplyr::setdiff(main.effect.names,j)
# y_tilde_2_list_temp <- lapply(beta_hat_next_list[which(converged==0)], function(i) y -
#                                   x[,j_prime_not_in_j, drop = F] %*% i[j_prime_not_in_j, , drop = F])
for (notconverged in which(converged==0)) {
y_tilde_2_list_temp[[notconverged]] <- y -
x[,j_prime_not_in_j, drop = F] %*% beta_hat_next_list[[notconverged]][j_prime_not_in_j, , drop = F]
}
# length(y_tilde_2_list_temp)
# mclapply is faster than lapply even with just two cores
term_2_temp_list_not_converged <- parallel::mclapply(seq_len(nlambda)[which(converged==0)], function(i)
as.matrix(
rowSums(
xtilde_mod(beta.main.effects = beta_hat_next_list[[i]][j_prime_not_in_j, , drop = F],
gamma.interaction.effects = gamma_hat_next_list[[i]],
interaction.names = interaction.names[-grep(j, interaction.names)],
data.main.effects = x[,j_prime_not_in_j, drop = F])
),
ncol = 1),
mc.cores = cores)
term_2_temp_list <- replace(term_2_temp_list, which(converged==0), term_2_temp_list_not_converged)
# this is of length nlambda.beta*nlambda.gamma i.e. one set of y's for each tuning parameter
y_tilde_2_list <- mapply("-", y_tilde_2_list_temp, term_2_temp_list, SIMPLIFY = F)
# j' less than j
j.prime.less <- index[which(index[,"index"] < index[which(index$main.effect.names == j),2]),
"main.effect.names"]
# the if conditions in term1 and term2 are to check if there are
# any variables greater or less than j
# lapply is faster than mclapply here
term_1_list_not_converged <- if (length(j.prime.less) != 0) {
lapply(seq_len(nlambda)[which(converged==0)], function(i)
x[,paste(j.prime.less,j,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j.prime.less,j,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.less, , drop = F]))} else
matrix(rep(0,length(beta_hat_next_list[which(converged==0)])), ncol = 1)
term_1_list <- replace(term_1_list, which(converged==0), term_1_list_not_converged)
# j' greater than j
j.prime.greater <- index[which(index[,"index"] > index[which(index$main.effect.names == j),2]),
"main.effect.names"]
term_2_list_not_converged <- if (length(j.prime.greater) != 0) {
lapply(seq_len(nlambda)[which(converged==0)], function(i)
x[,paste(j,j.prime.greater,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j, j.prime.greater,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.greater,,drop = F])) } else
matrix(rep(0,length(beta_hat_next_list[which(converged==0)])), ncol = 1)
term_2_list <- replace(term_2_list, which(converged==0), term_2_list_not_converged)
# lapply is faster than mclapply
x_tilde_2_list <- lapply(seq_len(length(term_1_list)),
function(i) x[,j, drop = F] +
term_1_list[[i]] + term_2_list[[i]])
# glmnet is giving weired results for this... and is slower than using my
# soft function. use this. non-parallel version is faster
# the result of this should give 1 beta for each tuningn parameter
# This calculates for all tuning parameters
beta_hat_next_list_j <- lapply(seq_len(length(x_tilde_2_list)), function(i)
soft(x = x_tilde_2_list[[i]],
y = y_tilde_2_list[[i]],
weight = adaptive_weights_list[[i]][j,,drop=F],
lambda = lambda_beta_list[[i]]))
# update beta_j for each tuning parameter but only those that
# have not converged
for (i in seq_len(nlambda)[which(converged==0)]) {
beta_hat_next_list[[i]][j,] <- beta_hat_next_list_j[[i]]
}
}
# Q[m+1,which(converged==0)] <- parallel::mcmapply(Q_theta,
#                               beta = beta_hat_next_list[which(converged==0)],
#                               gamma = gamma_hat_next_list[which(converged==0)],
#                               lambda.beta = lambda_beta_list[which(converged==0)],
#                               lambda.gamma = lambda_gamma_list[which(converged==0)],
#                               MoreArgs = list(x = x, y = y,
#                                               weights = adaptive.weights,
#                                               main.effect.names = main.effect.names,
#                                               interaction.names = interaction.names),
#                               mc.cores = cores)
Q[m+1,] <- parallel::mcmapply(Q_theta,
beta = beta_hat_next_list,
gamma = gamma_hat_next_list,
lambda.beta = lambda_beta_list,
lambda.gamma = lambda_gamma_list,
weights = adaptive_weights_list,
MoreArgs = list(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names),
mc.cores = cores)
#betas[,m+1,] <- beta_hat_next_list
#gammas[,m+1,] <- gamma_hat_next_list
delta <- abs(Q[m,] - Q[m+1,])/abs(Q[m,])
converged <- as.numeric(delta<threshold)
print(paste("Iteration:",m, ", Q(theta):",Q[m+1,2]))
print(paste(converged))
m = m + 1
beta_hat_previous_list <- beta_hat_next_list
# adaptive weight for each tuning parameter. currently this is the
# same for iterations, but I am coding it here
# for flexibility in case we want to change the weights at each iteration
# adaptive_weights_list <- lapply(x_tilde_list, function(i)
#     adaptive.weights[colnames(i),,drop = F])
#
# colnames(x_tilde_list[[1]])
adaptive_weights_list_not_converged <- lapply(seq_len(nlambda)[which(converged==0)],
function(i)
update_weights(betas = beta_hat_previous_list[[i]],
gammas = gamma_hat_previous_list[[i]],
main.effect.names = main.effect.names,
interaction.names = interaction.names))
adaptive_weights_list <- replace(adaptive_weights_list, which(converged==0), adaptive_weights_list_not_converged)
for (j in which(converged==0)) {
y_tilde_list[[j]] <- y - x[,main.effect.names,drop = F] %*% beta_hat_previous_list[[j]]
}
# calculate x_tilde for each beta vector corresponding to a diffent tuning parameter
# x_tilde_list <- lapply(beta_hat_previous_list[which(converged==0)],
#                        function(i) xtilde(interaction.names = interaction.names,
#                                           data.main.effects = x[,main.effect.names, drop = F],
#                                           beta.main.effects = i))
for (j in which(converged==0)) {
x_tilde_list[[j]] <- xtilde(interaction.names = interaction.names,
data.main.effects = x[,main.effect.names, drop = F],
beta.main.effects = beta_hat_previous_list[[j]])
}
# indices of the x_tilde matrices that have all 0 columns
zero_x_tilde <- which(sapply(x_tilde_list,
function(i) is.null(colnames(check_col_0(i)))))
# this will store the results but will be shorter than nlambda
gamma_hat_next_list_not_converged <- parallel::mclapply(seq_len(nlambda)[which(converged==0)],
function(i) {
if (i %in% zero_x_tilde) coef_zero_gamma_matrix else
as.matrix(coef(glmnet::glmnet(
x = x_tilde_list[[i]],
y = y_tilde_list[[i]],
penalty.factor = adaptive_weights_list[[i]][interaction.names,,drop=F],
lambda = lambda_gamma_list[[i]],
standardize = F, intercept = F))[-1,,drop = F])},
mc.cores = cores)
# k = 1
# for (j in which(converged==0)) {
#     gamma_hat_next_list[[j]] <- gamma_hat_next_list_tmp[[k]]
#     k = k + 1
# }
gamma_hat_next_list <- replace(gamma_hat_next_list, which(converged==0), gamma_hat_next_list_not_converged)
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# update beta (main effect parameter) step 4 of algortihm in Choi et al
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
beta_hat_next_list <- beta_hat_previous_list
for (j in main.effect.names) {
#j = "x5"
#print(paste(j))
# determine the main effects not in j
j_prime_not_in_j <- dplyr::setdiff(main.effect.names,j)
# y_tilde_2_list_temp <- lapply(beta_hat_next_list[which(converged==0)], function(i) y -
#                                   x[,j_prime_not_in_j, drop = F] %*% i[j_prime_not_in_j, , drop = F])
for (notconverged in which(converged==0)) {
y_tilde_2_list_temp[[notconverged]] <- y -
x[,j_prime_not_in_j, drop = F] %*% beta_hat_next_list[[notconverged]][j_prime_not_in_j, , drop = F]
}
# length(y_tilde_2_list_temp)
# mclapply is faster than lapply even with just two cores
term_2_temp_list_not_converged <- parallel::mclapply(seq_len(nlambda)[which(converged==0)], function(i)
as.matrix(
rowSums(
xtilde_mod(beta.main.effects = beta_hat_next_list[[i]][j_prime_not_in_j, , drop = F],
gamma.interaction.effects = gamma_hat_next_list[[i]],
interaction.names = interaction.names[-grep(j, interaction.names)],
data.main.effects = x[,j_prime_not_in_j, drop = F])
),
ncol = 1),
mc.cores = cores)
term_2_temp_list <- replace(term_2_temp_list, which(converged==0), term_2_temp_list_not_converged)
# this is of length nlambda.beta*nlambda.gamma i.e. one set of y's for each tuning parameter
y_tilde_2_list <- mapply("-", y_tilde_2_list_temp, term_2_temp_list, SIMPLIFY = F)
# j' less than j
j.prime.less <- index[which(index[,"index"] < index[which(index$main.effect.names == j),2]),
"main.effect.names"]
# the if conditions in term1 and term2 are to check if there are
# any variables greater or less than j
# lapply is faster than mclapply here
term_1_list_not_converged <- if (length(j.prime.less) != 0) {
lapply(seq_len(nlambda)[which(converged==0)], function(i)
x[,paste(j.prime.less,j,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j.prime.less,j,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.less, , drop = F]))} else
matrix(rep(0,length(beta_hat_next_list[which(converged==0)])), ncol = 1)
term_1_list <- replace(term_1_list, which(converged==0), term_1_list_not_converged)
# j' greater than j
j.prime.greater <- index[which(index[,"index"] > index[which(index$main.effect.names == j),2]),
"main.effect.names"]
term_2_list_not_converged <- if (length(j.prime.greater) != 0) {
lapply(seq_len(nlambda)[which(converged==0)], function(i)
x[,paste(j,j.prime.greater,sep = ":")] %*%
(gamma_hat_next_list[[i]][paste(j, j.prime.greater,sep = ":"),, drop = F] *
beta_hat_next_list[[i]][j.prime.greater,,drop = F])) } else
matrix(rep(0,length(beta_hat_next_list[which(converged==0)])), ncol = 1)
term_2_list <- replace(term_2_list, which(converged==0), term_2_list_not_converged)
# lapply is faster than mclapply
x_tilde_2_list <- lapply(seq_len(length(term_1_list)),
function(i) x[,j, drop = F] +
term_1_list[[i]] + term_2_list[[i]])
# glmnet is giving weired results for this... and is slower than using my
# soft function. use this. non-parallel version is faster
# the result of this should give 1 beta for each tuningn parameter
# This calculates for all tuning parameters
beta_hat_next_list_j <- lapply(seq_len(length(x_tilde_2_list)), function(i)
soft(x = x_tilde_2_list[[i]],
y = y_tilde_2_list[[i]],
weight = adaptive_weights_list[[i]][j,,drop=F],
lambda = lambda_beta_list[[i]]))
# update beta_j for each tuning parameter but only those that
# have not converged
for (i in seq_len(nlambda)[which(converged==0)]) {
beta_hat_next_list[[i]][j,] <- beta_hat_next_list_j[[i]]
}
}
# Q[m+1,which(converged==0)] <- parallel::mcmapply(Q_theta,
#                               beta = beta_hat_next_list[which(converged==0)],
#                               gamma = gamma_hat_next_list[which(converged==0)],
#                               lambda.beta = lambda_beta_list[which(converged==0)],
#                               lambda.gamma = lambda_gamma_list[which(converged==0)],
#                               MoreArgs = list(x = x, y = y,
#                                               weights = adaptive.weights,
#                                               main.effect.names = main.effect.names,
#                                               interaction.names = interaction.names),
#                               mc.cores = cores)
Q[m+1,] <- parallel::mcmapply(Q_theta,
beta = beta_hat_next_list,
gamma = gamma_hat_next_list,
lambda.beta = lambda_beta_list,
lambda.gamma = lambda_gamma_list,
weights = adaptive_weights_list,
MoreArgs = list(x = x, y = y,
main.effect.names = main.effect.names,
interaction.names = interaction.names),
mc.cores = cores)
#betas[,m+1,] <- beta_hat_next_list
#gammas[,m+1,] <- gamma_hat_next_list
delta <- abs(Q[m,] - Q[m+1,])/abs(Q[m,])
converged <- as.numeric(delta<threshold)
print(paste("Iteration:",m, ", Q(theta):",Q[m+1,2]))
print(paste(converged))
m = m + 1
beta_hat_previous_list <- beta_hat_next_list
# adaptive weight for each tuning parameter. currently this is the
# same for iterations, but I am coding it here
# for flexibility in case we want to change the weights at each iteration
# adaptive_weights_list <- lapply(x_tilde_list, function(i)
#     adaptive.weights[colnames(i),,drop = F])
#
# colnames(x_tilde_list[[1]])
adaptive_weights_list_not_converged <- lapply(seq_len(nlambda)[which(converged==0)],
function(i)
update_weights(betas = beta_hat_previous_list[[i]],
gammas = gamma_hat_previous_list[[i]],
main.effect.names = main.effect.names,
interaction.names = interaction.names))
adaptive_weights_list <- replace(adaptive_weights_list, which(converged==0), adaptive_weights_list_not_converged)
rm(list = ls())
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
system.time(res <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 50 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 5, cores = 3))
133/60
rm(list = ls())
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
system.time(res <- shim_multiple(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 3))
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 3))
res$beta
res$Q
res$gamma
res$gamma[[1]]
res2$gamma[[1]]
res2$gamma[[37]]
res2$gamma[[30]]
res$gamma[[30]]
res$Q  %>% dim
rm(list = ls())
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 3))
rm(list = ls())
options(scipen=999, digits = 10)
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
x = X; y = Y; main.effect.names = main_effect_names;
interaction.names = interaction_names;
lambda.beta = NULL ; lambda.gamma = NULL
threshold = 1e-5 ; max.iter = 500 ; initialization.type = "ridge";
nlambda.gamma = 5; nlambda.beta = 10; cores = 2
rm(list = ls())
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 3))
res2$converged
res2$beta
betas <- matrix(unlist(res2$beta), ncol = length(res2$beta), byrow = TRUE)
dim(betas)
gammas <- matrix(unlist(res2$gamma), ncol = length(res2$gamma), byrow = TRUE)
dim(gammas)
matplot(t(betas), type="l")
matplot(t(gammas), type="l")
matplot(res$Q , type="l")
matplot(res2$Q , type="l")
rm(list = ls())
source("packages.R")
source("data.R")
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
source("functions.R")
"%ni%" <- Negate("%in%")
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 3))
250/60
betas <- matrix(unlist(res2$beta), ncol = length(res2$beta), byrow = TRUE)
dim(betas)
gammas <- matrix(unlist(res2$gamma), ncol = length(res2$gamma), byrow = TRUE)
dim(gammas)
matplot(t(betas), type="l")
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 4))
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 1))
167/60
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 50, nlambda.beta = 1, cores = 1))
# 1 core is faster than more ...
system.time(res2 <- shim_multiple_faster(x = X, y = Y, main.effect.names = main_effect_names,
interaction.names = interaction_names,
lambda.beta = NULL , lambda.gamma = NULL,
threshold = 1e-5 , max.iter = 100 , initialization.type = "ridge",
nlambda.gamma = 5, nlambda.beta = 10, cores = 1))
rm(list=ls())
rm(list=ls())
library(readxl)
library(data.table)
library(magrittr)
library(pander)
library(DT)
library(lubridate)
library(Epi)
library(ggplot2)
source("functions.R")
DT <- fread("data/CompleteData01Apr15.csv")
